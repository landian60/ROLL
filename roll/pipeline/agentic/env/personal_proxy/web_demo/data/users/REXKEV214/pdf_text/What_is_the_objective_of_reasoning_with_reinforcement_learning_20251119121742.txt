What is the objective of reasoning with reinforcement
learning?
Damek Davis∗Benjamin Recht†
October 16, 2025
Abstract
We show that several popular algorithms for reinforcement learning in large lan-
guage models with binary rewards can be viewed as stochastic gradient ascent on a
monotone transform of the probability of a correct answer given a prompt. In particu-
lar, the transformation associated with rejection sampling algorithms is the logarithm
and that associated with the GRPO algorithm is the arcsine of the square root.
1 Introduction
Though traditionally associated with sophisticated tree search and approximate dynamic
programming, reinforcement learning takes on a unique character in the post-training of
large language models. As a tool for adjusting models to align with human preferences or
gain proficiency in certain test-taking domains, reinforcement learning algorithms tend to
all work like the following meta-algorithm:
1. Sample a bunch of prompts from a corpus.
2. Have the model generate a set of responses to the prompts.
3. Have some external source, be it crowd workers or a verification engine, label the
answers as good or bad.
4. Fine-tune the model based on the triples of prompts, responses, and labels.
We will refer to this underspecified procedure as Meta-Algorithm 1.
Many algorithms have been proposed that run some sort of variant of the meta-algorithm
(see references [8-621]). In this note, we argue that they can all be interpreted as stochastic
gradient methods on closely related objective functions. Namely, they aim to find a model
∗Department of Statistics and Data Science, The Wharton School, University of Pennsylvania;www.
damekdavis.com. Work supported by NSF DMS award 2047637.
†Department of Electrical Engineering and Computer Sciences, University of California, Berkeleyhttps:
//people.eecs.berkeley.edu/ ~brecht/.
1arXiv:2510.13651v1  [cs.LG]  15 Oct 2025

that maximizes a monotonically increasing function of the probability of getting a correct
answer conditioned on the prompt.
This serves as a unifying, simple way to look at reinforcement learning applied to large
language models and to compare different algorithms. Williams’ REINFORCE algorithm
and other policy gradient algorithms have enough degrees of freedom that understanding
what they do when applied to particular optimization problems is not always transparent.
When applied to LLMs, different implementations of Meta-algorithm 1 can all be viewed as
maximizing closely related objective functions.
Moreover, Meta-algorithm 1 highlights that the base model must already perform non-
trivially in order for fine tuning on correct answers alone to be profitable. If neither the
base model nor the engineer can generate correct answers, then the fine-tuning algorithms
cannot make progress. No sophisticated implementation of policy gradient can circumvent
this constraint.
In what follows, after recalling some facts from supervised and reinforcement learning,
we present a precise form of Meta-Algorithm 1 in Section 3, which aims to maximize the
scaled probability of correct responses to prompts via stochastic gradient ascent. We then
proceed to show how popular post-training algorithms like REINFORCE, rejection sampling
fine-tuning, and GRPO are all special cases of our proposed stochastic gradient method.
2 Background: Supervised Learning and Williams’ RE-
INFORCE algorithm.
It’s first worth reminding ourselves of what a supervised learning update looks like for prob-
abilistic modeling. Say our goal is to fit a conditional distributionp θ(y|x) to a set ofn
example input-output pairs (x i, yi). A popular approach to this problem maximizes the
global objective function
nX
i=1logp θ(yi|xi).(2.1)
One can motivate this as seeking the maximum likelihoodθgiven the observed input-output
data. A stochastic gradient of this loss for exampleiis thus
∇θlogp θ(yi|xi)
We note this because this quantity will appear throughout this work. Most of reinforcement
learning when applied to LLMs looks like supervised learning, with the catch that the outputs
yiare generated by the model being optimized.
Let’s see how reinforcement learning updates relate to these supervised learning updates.
To get there, we review Williams’ REINFORCE algorithm (Williams, 1992) in full gener-
ality. REINFORCE is based on the following straightforward observation. Letp θ(z) be a
probability distribution inz. Define the function
Φ(θ) =E z∼pθ(·)[F(z)].
Then forzsampled fromp θ
F(z)∇ θlogp θ(z) (2.2)
2

is an unbiased estimate of∇ θΦ(θ). The REINFORCE algorithm is the stochastic gradient
method where the gradient estimate is the concatenation of the unbiased partial derivative
estimates of (2.2).
The key to deriving this expression is the trivial equality from calculus
∇θlogp θ(z) =∇θpθ(z)
pθ(z).
Hence, by multiplying and dividing gradients of probability distributions, we can derive
stochastic gradients that are proportional to the gradients of the logarithms of densities. We
will make use of such analyses throughout.
The REINFORCE algorithm is general, and the distributionp θ(·) does not need to be
a conditional distribution like it is in supervised learning. However, reinforcement learning
applied to problems of the form
maximizenX
i=1Ey∼pθ(·|xi)[R(y, x i)].
will have updates that look like supervised learning when the rewards are binary valued. We
will explore several examples of this in the next sections.
3 Reinforcement Learning for Reasoning
Consider the simplest RL problem for large language models. We have a base modelπ θfrom
which we can generateanswers,y, toquestions,x. Herexfunctions as the prompt andy,
the response, is a random sample from a probability distributionπ θ(· |x), whereθdenotes
the parameters of the probabilistic model. We assume that we have some computational
means of determining when an answeryis a “correct” response to the questionx. For each
questionx, we denote the set of correct answers byC(x).
The goal is to fine-tune the probabilistic modelπ θso that it frequently correctly answers
questions from a corpus of questionsQ. In what follows, we show how several different
versions of Meta-Algorithm 1 can be interpreted as performing stochastic gradient ascent on
the objective
Jh(θ) :=E x∼Q
h
X
y∈C(x)πθ(y|x)

,(3.1)
for some monotonically increasing functionh. The term
pθ(C|x) :=X
y∈C(x)πθ(y|x) (3.2)
is the probability a sample fromπ θ(· |x) is labeled as correct.
In practice, Meta-Algorithm 1 is implemented through the following more concrete instan-
tiation, which we call Algorithm 1, because we refuse to come up with a cutesy, impenetrable
four letter acronym.
3

1. Select a questionxfrom the corpusQ.
2. SampleManswers from the current model.
3. Based on the evaluation of each responsey i, compute a per-sample weightZ i.
4. Fine-tune the model with a supervised learning update
θ←θ+η1
MMX
i=1Zi∇θlogπ θ(yi|x).(3.3)
The weightsZ iare often calledadvantagesin the RL literature. Typically, algorithm
designers motivate the choice ofZ iby appealing to variance reduction. Instead, we show
that typical choices ofZ iin fact induce different cost functionsh Msuch that
Ey1:M"
1
MMX
i=1Zi∇θlogπ θ(yi|x)#
:=∇ θhM(pθ(C|x)) (3.4)
This means that our choice of advantage determines the objectiveJ hin (3.1) that we op-
timize. For example, Williams’ version of REINFORCE, affectionately dubbed “vanilla”
REINFORCE, uses advantageZ i= 1 yi∈C(x) which clearly leads to objectiveh(t) =t. That
is, it attempts to maximize the probability of correct answers, averaged over the questions.
More interestingly, we will show that using rejection sampling to find correct answers
corresponds to a function close toh(t) = log(t). In addition, the popular algorithm GRPO
algorithm corresponds to an objective close toh(t) = arcsin(√
t). We also ask the converse
question: given a functionh, how should we choose the weightsZ iso that the inducedh M
is close toh. It turns out the answer is related to the Bernstein polynomial expansion ofh;
we discuss this in Section 7.
0.00 0.25 0.50 0.75 1.00
t0.000.250.500.751.00
yComparison: 2arcsin(t), t, and ln(t)+1
2arcsin(t)
t
ln(t)+1
Figure 1: GRPO loss vs. REINFORCE loss vs. log loss
4

4 Which functions do the reweightings induce?
We will consider a specific family of weightsZ iwhich are conditionally linear inR i. Specif-
ically, define rewardsR i= 1 yi∈C(x) and the leave-one-out total rewardsS i=P
j̸=iRi. The
weights we consider in this work are all of the form:
Zi= (1−R i)aSi+R ibSi (4.1)
wheres7→a sands7→b sare arbitrary (measurable) functions ofs. We claim that that such
weights always induce a transform of the form.
hM(t) =1
MM−1X
s=0(bs−as)It(s+ 1, M−s),(4.2)
wheret7→I t(s+ 1, M−s) is the regularized incomplete beta function.
To prove this, note that each termZ i∇θlogπ θ(yi|x) has the same distribution. There-
fore, we must only show
Eyi∼πθ(·|x)[Zi∇θlogπ θ(yi|x)] =∇ θhM(pθ(C|x)) =h′
M(pθ(C|x))∇ θpθ(C|x).
We can argue this by conditioning onS i=s. Indeed, fixs, define a scalarϕ s:=b s−as, and
define a functiong s(y) which is equal tob swheny∈C(x) and equal toa swheny /∈C(x).
Then conditioned onS i=s, we haveg s(y) =Z i. Moreover, sincey iis independent ofS i,
E[Z i∇θlogπ θ(yi|x)|S i=s] =X
ygs(y)∇ θπθ(y|x) =ϕ sX
y∈C(x)∇θπθ(y|x) =ϕ s∇θpθ(C|x),
Averaging overS i∼Bin(M−1, p), we find that
E[Z i∇θlogπ θ(yi|x)] =κ(p)∇ θpθ(C|x),
where
κ(t) :=M−1X
s=0ϕsM−1
s
ts(1−t)M−1−s.
Sinced
dtIt(s+ 1, M−s) =M M−1
s
ts(1−t)M−1−s, we haveh′
M(t) =κ(t), as desired.
In the next two sections, we consider the weights for rejection sampling and GRPO.
5 Rejection sampling
Consider the special case of Algorithm 1 (see e.g. Xiong et al. (2025)): SampleManswers
from the model, letVdenote the set of correct answers. Then use the gradient estimator:
1
|V|X
y∈V∇θlogπ θ(y|x).
5

If you receive no correct answers, skip the step. One can see that the choice of advantage
here is simply:Z i=R iM/(S i+ 1).and thus, the induced cost function is:
hM(t) =M−1X
s=01
s+ 1It(s+ 1, M−s),
Note that this function is close to log(t), and gets closer asMapproaches infinity. In fact,
one can show that
hM(t) = log(t) +H M+∞X
r=M+1(1−t)r
r,
whereH Mis theMth harmonic number; see Figure 2.
e6
e3
e2
e1
1
t5
4
3
2
1
01hM(t)(HM 1) vs. ln(t)+1
M=2
M=16
M=256
ln(t)+1
Figure 2: Functionh Minduced from rejection sampling.
One might wonder whether it’s possible to actually target the objectiveh(t) = log(t).
This can be achieved by rejection sampling, which falls slightly outside of Algorithm 1.
Indeed, consider the following algorithm:
1. Select a question from the corpusQ.
2. For a chosen integer,B, sample answers from the model until you observeBcorrect
responses. LetVdenote this set of correct answers.
3. Fine-tune the model with a supervised learning update
θ←θ+η1
BX
y∈V∇θlogπ θ(y|x).
A similar algorithm using rejection sampling was proposed for post-training by Xiong et al.
(2025). The algorithm as written here is in fact stochastic gradient ascent with an unbiased
estimate of the gradient ofJ log. We use an analysis that mimics the log trick used to derive
6

Williams’ REINFORCE algorithm. Note that
∇θlogX
y∈C(x)πθ(y|x) =P
y∈C(x)∇θπθ(y|x)P
y∈C(x) πθ(y|x)
=P
y∈C(x) πθ(y|x)∇ θlogπ θ(y|x)P
y∈C(x) πθ(y|x)
=E[∇ θlogπ θ(y|x)|y∈C(x), x].
Hence, samplingyusing rejection sampling yields an unbiased stochastic gradient of the cost
Jlog.
Note that this even gives a new way to implement supervised learning, albeit one that
is not very practical. For a given example (x i, yi), you can sample fromyfromp θ(·|xi) until
you find one withy=y i. At this point, update the model in the direction of the gradient of
logp θ(y|x i). This algorithm maximizes the standard log-loss objective (2.1).
However, in the reinforcement learning context, the global objectiveJ logdoes not have
a natural interpretation as a maximum likelihood estimator unless there is a single correct
answer inC(x). The objectiveJ logis more analogous to the multilabel problem in supervised
learning where many labels can be counted as correct for a particular example data-point.
As we discuss in the conclusion, it’s not clear whether there is a rigorous justification for
using the logarithmic scaling in reinforcement learning contexts.
6 GRPO
The GRPO algorithm (Shao et al., 2024) is another renormalization of the sampled gradients.
GRPO normalizes by the square root of the variance of the received rewards. This amplifies
the impact of each update. Indeed, after samplingy 1, . . . , y M∼π θ(· |x), one forms the
stochastic gradient estimator:
1
MMX
i=1 
Ri−1
MPM
i=1Rip
Var({R i}i) +ε!
∇θlogπ θ(yi|x) for someε >0. (6.1)
Which loss does this correspond to? Let us first note that conditioned onS i=s, the
weightingZ iis indeed linear in the indicatorR i
Z:
i=Ri−1
MPM
i=1Rip
Var({R i}i) +ε= (1−R i)−qsp
qs(1−q s) +ε+R i1−q s+1p
qs+1(1−q s+1) +ε
whereq s=s/M. From this expression, the objective induced byZ iis
JM,ε(θ) =E x∼Q
hM,ε
X
y∈C(x)πθ(y|x)

,
7

whereh M,εis defined via (4.2):
hM,ε(t) :=1
MM−1X
s=0 
1−q s+1p
qs+1(1−q s+1) +ε+qsp
qs(1−q s) +ε!
It(s+ 1, M−s);
This function is admittedly opaque, but one can get a sense of what it’s targeting by con-
sidering an idealized setting whereε= 0 and we replace the sample mean and variance by
their population values.
Indeed, let us consider the functionh(t) = 2 arcsin√
tand differentiate:
∇θh(pθ(C|x)) =∇θpθ(C|x)p
pθ(C|x)(1−p θ(C|x))
=Ey∼πθ(·|x)[R(y, x)∇ θlogπ θ(y|x)]p
Vary∼πθ(·|x)(R(y, x))
=E y∼πθ(y|x)" 
R(y, x)−E y∼πθ(·|x)[R(y, x)]p
Vary∼πθ(·|x)(R(y, x))!
∇θlogπ θ(y|x)#
,
where the third equality uses the fact that the expected value of the log derivative under
πθ(· |x) is zero. Figure 3 plots the behavior ofh M,ε/hM,ε(1). Reassuringly, we see that for
largeMand smallε, the function tends toh(t) =2
πarcsin√
t. Asεincreases,h M,εbehaves
more likeh(t) =t.
0.00 0.25 0.50 0.75 1.00
t0.000.250.500.751.00=108
M=2
M=16
M=256
(2/)arcsin(t)
0.00 0.25 0.50 0.75 1.00
t0.000.250.500.751.00M=128
=108
=101
(2/)arcsin(t)
Normalized hM,(t)
Figure 3: The effect ofMandεon the functionh M,ε/hM,ε(1).
So again, like the rejection sampling algorithm from Section 5, the GRPO algorithm is
optimizing a monotone rescaling of the probability of achieving a correct answer. Why is
this a good scaling function? As we discuss in the conclusion, we’re not sure.
7 What do we learn?
This note only aims to provide language model researchers more clarity and flexibility in
designing fine-tuning methods, so we provide no concrete recommendations. In fact, a variety
8

of other sampling and reweighting methods can be derived using the approach of this note.
For example, replacing the standard deviation by the variance in GRPO yields a function
close to the log odds rescaling:h(t) = log(t/(1−t)). Other rescalings based on normalizing
by the pdf of the beta distribution have also been considered, e.g., in Xiao et al. (2025)
and their associated loss functions which approximate the corresponding cdfs of the beta
distribution can be deduced from 4.
What if maximizing log-odds is what you want? Or what if there’s some other scaling
functionhyou like? In that case, we can provide you a recipe for constructing a gradient
estimator. The process is straightforward: the derivative ofh Min (4.2) is a Bernstein
polynomial, which provides a basis in which to approximate any continuous function:
h′
M(t) =M−1X
s=0(bs−as)M−1
s
ts(1−t)M−1−s(7.1)
Then one simply needs to choosea sandb sappropriately to best approximateh′byh′
M.
For example, ifhis a polynomial of degree≤M, one may choose these coefficients so that
h′
M=h′and henceh M=h+ const. Ifhis an arbitrary smooth function, one can instead
choose coefficients such thatb s−as=h′(s/(M−1)). With this choice of coefficients,h M
converges uniformly toh+ const on [0,1]. One can even prove a rate of convergence if
e.g.,hisCpforp≥3. In this case the derivativeh′
Mconverges in the sup norm at a rate
1/M, and the same result holds forh Mby the fundamental theorem of calculus (Adell and
C´ ardenas-Morales, 2022).
We do not see a reason to prefer anyhmore than the identity. For a corpus consisting
of a single question, each choice ofh(t) induces an identical problem, sincehis monotone.
Unfortunately, it is hard to guess the practical effects of monotone rescaling of the objective
function in this manner when there are multiple questions in the corpus. In fact, if we allow
for an expressive enough representation ofπ θand globally maximizeJ h, the choice ofhis
superfluous (though it may affect optimization dynamics): For any monotone scaling, the
optimal choice is to place a total probability mass of 1 onC(x) and 0 everywhere else. This
is analogous to how hinge and logistic loss on linearly separable data are minimized at the
same halfspaces.
Thus, arguing whether GRPO or REINFORCE is best is like arguing whether log loss
is better than hinge loss for classification problems. There as well, Logistic regression and
support vector machines differ only in a monotonic rescaling of errors in the objective func-
tion. Statistically, both return comparable classifiers with sufficient data (See, for example,
Bartlett et al. (2006)). But neither loss has magical properties, and the best loss varies by
task. In the same vein, the best rescaling for RL finetuning will be context dependent, since
all such algorithms are chasing closely related objectives.
References
J. A. Adell and D. C´ ardenas-Morales. Asymptotic and non-asymptotic results in the ap-
proximation by bernstein polynomials.Results in Mathematics, 77(4):166, 2022.
9

P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds.
Journal of the American Statistical Association, 101(473):138–156, 2006.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.arXiv preprint
arXiv:1412.6980, 2014.
Z. Shao et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language
models.arXiv:2402.03300, 2024.
R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-
ment learning.Machine Learning, 8:229–256, 1992. doi: 10.1007/BF00992696.
C. Xiao, M. Zhang, and Y. Cao. Bnpo: Beta normalization policy optimization.arXiv
preprint arXiv:2506.02864, 2025.
W. Xiong, J. Yao, Y. Xu, B. Pang, L. Wang, D. Sahoo, J. Li, N. Jiang, T. Zhang, C. Xiong,
et al. A minimalist approach to llm reasoning: from rejection sampling to reinforce.arXiv
preprint arXiv:2504.11343, 2025.
10